{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scipy scikit-learn spacy gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import spacy\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 11.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy English model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column groups\n",
    "short_text_columns = [\n",
    "    \"api_usage_pre\", \"tableau_usage_pre\", \"ml_application_pre\", \"persona_explanation_pre\",\n",
    "    \"tools_usage_post\", \"api_usage_post\", \"ml_application_post\"\n",
    "]\n",
    "long_text_columns = [\n",
    "    \"data_collection_explanation_post\", \"data_analysis_explanation_post\",\n",
    "    \"persona_building_explanation_post\", \"evaluation_explanation_post\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_with_glove(sentence, model, vector_size=50):\n",
    "    \"\"\"Convert a tokenized sentence into a GloVe word embedding vector.\"\"\"\n",
    "    words = [token.text for token in nlp(sentence) if token.text in model]\n",
    "    if not words:\n",
    "        return np.zeros(vector_size)  # Return zero vector if no known words are found\n",
    "    return np.mean([model[word] for word in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_survey(file_name, use_count_vectorizer=False, save=True):\n",
    "    \"\"\"Vectorizes a survey dataset using TF-IDF (or CountVectorizer) and GloVe embeddings, and saves output.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_name)\n",
    "        print(f\"Processing survey: {file_name}\")\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        missing_cols = [col for col in short_text_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing short_text_columns: {missing_cols}\")\n",
    "\n",
    "        missing_cols = [col for col in long_text_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing long_text_columns: {missing_cols}\")\n",
    "\n",
    "        # Use either TF-IDF or CountVectorizer based on parameter\n",
    "        if use_count_vectorizer:\n",
    "            vectorizer = CountVectorizer()\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(norm=None)  # Prevents negative values\n",
    "\n",
    "        short_response_vectors = vectorizer.fit_transform(\n",
    "            df[short_text_columns].fillna(\"\").agg(\" \".join, axis=1)\n",
    "        )\n",
    "\n",
    "        # GloVe Embeddings for long responses\n",
    "        df[\"glove_vector\"] = df[long_text_columns].fillna(\"\").agg(\" \".join, axis=1).apply(\n",
    "            lambda x: vectorize_sentence_with_glove(x, glove_model)\n",
    "        )\n",
    "\n",
    "        # Convert GloVe vectors to sparse format\n",
    "        glove_vectors = csr_matrix(np.vstack(df[\"glove_vector\"]))\n",
    "\n",
    "        # Stack both feature matrices\n",
    "        final_feature_matrix = hstack([short_response_vectors, glove_vectors])\n",
    "\n",
    "        print(f\"Vectorization complete for: {file_name}, Shape: {final_feature_matrix.shape}\")\n",
    "\n",
    "        # ✅ Save vectorized output if enabled\n",
    "        if save:\n",
    "            output_file = file_name.replace(\".csv\", \"_vectorized.npz\")\n",
    "            save_npz(output_file, final_feature_matrix)\n",
    "            print(f\"Saved vectorized features to: {output_file}\")\n",
    "\n",
    "        return final_feature_matrix\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
