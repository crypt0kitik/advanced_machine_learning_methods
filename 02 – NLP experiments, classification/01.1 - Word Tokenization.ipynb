{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had an issue with NLTK\n",
    "# that is I opted for spacy\n",
    "import spacy\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and load spaCy model\n",
    "def download_spacy_model():\n",
    "    try:\n",
    "        spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(\"Downloading 'en_core_web_sm' model...\")\n",
    "        subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
    "        print(\"Download complete!\")\n",
    "\n",
    "# Ensure spaCy model is available\n",
    "download_spacy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization(text):\n",
    "    \"\"\"\n",
    "    Function to tokenize text into words using spaCy.\n",
    "    :param text: str, input text\n",
    "    :return: list, tokenized words\n",
    "    \"\"\"\n",
    "    doc = nlp(text)  # Process the text using spaCy\n",
    "    return [token.text for token in doc]  # Extract word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"all_numeric_survey.csv\" \n",
    "text_columns = [\"tableau_usage_pre\", \"api_usage_pre\", \"ml_application_pre\",\n",
    "                    \"persona_explanation_pre\", \"api_usage_pre\", \"ml_application_pre\",\n",
    "                    \"data_collection_explanation_post\", \"data_analysis_explanation_post\", \"persona_building_explanation_post\",\n",
    "                    \"evaluation_explanation_post\", \"tools_usage_post\", \"api_usage_post\",\n",
    "                    \"ml_application_post\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Processed dataset saved as dataset_word_token.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(file_path)  # Load CSV file\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "\n",
    "    # Apply lemmatization to each text column\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:  # Ensure column exists before processing\n",
    "            df[col] = df[col].astype(str).apply(word_tokenization)\n",
    "\n",
    "    # Save the processed dataset\n",
    "    output_file = \"dataset_word_token.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed dataset saved as {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date_pre</th>\n",
       "      <th>end_date_pre</th>\n",
       "      <th>ip_address_pre</th>\n",
       "      <th>duration_sec_pre</th>\n",
       "      <th>response_id_pre</th>\n",
       "      <th>LocationLatitude_pre</th>\n",
       "      <th>LocationLongitude_pre</th>\n",
       "      <th>DistributionChannel_pre</th>\n",
       "      <th>UserLanguage_pre</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>age_pre</th>\n",
       "      <th>gender_pre</th>\n",
       "      <th>occupation_pre</th>\n",
       "      <th>teaching_marketing_pre</th>\n",
       "      <th>teaching_experience_pre</th>\n",
       "      <th>learning_style_pre</th>\n",
       "      <th>learning_format_pre</th>\n",
       "      <th>interaction_preference_pre</th>\n",
       "      <th>trusted_learning_method_pre</th>\n",
       "      <th>ai_familiarity_pre</th>\n",
       "      <th>ddp_familiarity_pre</th>\n",
       "      <th>data_sources_pre</th>\n",
       "      <th>persona_definition_pre</th>\n",
       "      <th>interactive_persona_pre</th>\n",
       "      <th>data_driven_persona_pre</th>\n",
       "      <th>dynamic_persona_pre</th>\n",
       "      <th>tableau_usage_pre</th>\n",
       "      <th>api_usage_pre</th>\n",
       "      <th>ml_application_pre</th>\n",
       "      <th>persona_explanation_pre</th>\n",
       "      <th>confirmation_pre</th>\n",
       "      <th>start_date_post</th>\n",
       "      <th>end_date_post</th>\n",
       "      <th>ip_address_post</th>\n",
       "      <th>duration_sec_post</th>\n",
       "      <th>response_id_post</th>\n",
       "      <th>LocationLatitude_post</th>\n",
       "      <th>LocationLongitude_post</th>\n",
       "      <th>DistributionChannel_post</th>\n",
       "      <th>UserLanguage_post</th>\n",
       "      <th>data_collection_explanation_post</th>\n",
       "      <th>data_analysis_explanation_post</th>\n",
       "      <th>persona_building_explanation_post</th>\n",
       "      <th>evaluation_explanation_post</th>\n",
       "      <th>ai_familiarity_post</th>\n",
       "      <th>ddp_familiarity_post</th>\n",
       "      <th>data_sources_post</th>\n",
       "      <th>persona_definition_post</th>\n",
       "      <th>interactive_persona_post</th>\n",
       "      <th>data_driven_persona_post</th>\n",
       "      <th>dynamic_persona_post</th>\n",
       "      <th>tools_usage_post</th>\n",
       "      <th>api_usage_post</th>\n",
       "      <th>ml_application_post</th>\n",
       "      <th>engagement_experience_post</th>\n",
       "      <th>interaction_quality_post</th>\n",
       "      <th>communication_clarity_post</th>\n",
       "      <th>trustworthiness_post</th>\n",
       "      <th>emotional_response_post</th>\n",
       "      <th>naturalness_post</th>\n",
       "      <th>effectiveness_post</th>\n",
       "      <th>comfort_level_post</th>\n",
       "      <th>personalization_post</th>\n",
       "      <th>mental_effort_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2/13/25 0:10</td>\n",
       "      <td>2/13/25 0:16</td>\n",
       "      <td>193.166.113.18</td>\n",
       "      <td>372</td>\n",
       "      <td>R_8GBoA0i1k6yogS1</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, do, not, know]</td>\n",
       "      <td>[[, ', I, ', ,, ', do, ', ,, ', not, ', ,, ', ...</td>\n",
       "      <td>[[, ', I, ', ,, ', do, ', ,, ', not, ', ,, ', ...</td>\n",
       "      <td>[I, do, not, know]</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:56:11</td>\n",
       "      <td>2025-02-13 1:18:59</td>\n",
       "      <td>193.166.113.18</td>\n",
       "      <td>1368</td>\n",
       "      <td>R_8QL21bpDcdbdtJ4</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[to, collect, studens, ', demographic, data, ,...</td>\n",
       "      <td>[First, the, data, must, be, cleaned, ., to, a...</td>\n",
       "      <td>[After, clustering, and, combining, ,, the, pe...</td>\n",
       "      <td>[The, evaluation, can, be, done, e.g., ,, with...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[To, communicate, the, personas, to, the, user...</td>\n",
       "      <td>[Because, relevant, data, can, be, obtained, f...</td>\n",
       "      <td>[They, can, be, used, e.g., ,, in, clustering,...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/13/25 0:09</td>\n",
       "      <td>2/13/25 0:16</td>\n",
       "      <td>193.166.113.32</td>\n",
       "      <td>409</td>\n",
       "      <td>R_8QG2UV0Zv0fKVB5</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1,3,4</td>\n",
       "      <td>2,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, do, not, know, these, tools, ,, so, I, can...</td>\n",
       "      <td>[[, ', I, ', ,, ', have, ', ,, ', no, ', ,, ',...</td>\n",
       "      <td>[[, ', I, ', ,, ', have, ', ,, ', no, ', ,, ',...</td>\n",
       "      <td>[I, have, no, clue, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:56:02</td>\n",
       "      <td>2025-02-13 1:13:21</td>\n",
       "      <td>193.166.113.32</td>\n",
       "      <td>1039</td>\n",
       "      <td>R_8QrAaCxxXwbpxvL</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[User, analytics, :, Google, analytics, ,, CRM...</td>\n",
       "      <td>[Segmentation, can, be, performed, so, that, e...</td>\n",
       "      <td>[One, way, to, create, student, personas, it, ...</td>\n",
       "      <td>[1, ), I, have, not, been, teaching, marketing...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[To, illustrate, (, and, update, ), static, pe...</td>\n",
       "      <td>[I, have, no, clue, what, is, API, .]</td>\n",
       "      <td>[To, create, dynamic, personas, .]</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2/13/25 0:09</td>\n",
       "      <td>2/13/25 0:17</td>\n",
       "      <td>193.166.113.31</td>\n",
       "      <td>462</td>\n",
       "      <td>R_8tHW5RDZd4yBkcx</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1,3</td>\n",
       "      <td>3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1,3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[I, do, n't, know, .]</td>\n",
       "      <td>[[, ', I, ', ,, ', do, ', ,, \", n't, \", ,, ', ...</td>\n",
       "      <td>[[, ', I, ', ,, ', do, ', ,, \", n't, \", ,, ', ...</td>\n",
       "      <td>[I, do, n't, know, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 10:06:34</td>\n",
       "      <td>2025-02-13 10:19:38</td>\n",
       "      <td>193.166.113.31</td>\n",
       "      <td>784</td>\n",
       "      <td>R_2kRKZFGiWHUQidw</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, need, quantitative, data, ,, which, can, b...</td>\n",
       "      <td>[Statistical, analyses, are, required, ., Thes...</td>\n",
       "      <td>[In, this, stage, ,, crafting, the, persona, p...</td>\n",
       "      <td>[This, is, a, crucial, step, in, the, persona,...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[To, visualise, the, persona, .]</td>\n",
       "      <td>[I, do, nt, know, what, API, stands, for, .]</td>\n",
       "      <td>[I, do, n't, know, .]</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2/13/25 0:11</td>\n",
       "      <td>2/13/25 0:17</td>\n",
       "      <td>193.166.113.21</td>\n",
       "      <td>355</td>\n",
       "      <td>R_8462bTis2cTNcii</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1,2,3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[, ', nan, ', ]]</td>\n",
       "      <td>[[, ', nan, ', ]]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:55:51</td>\n",
       "      <td>2025-02-13 1:11:01</td>\n",
       "      <td>193.166.113.21</td>\n",
       "      <td>909</td>\n",
       "      <td>R_2M3diy3SR17ual3</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Data, sources, :, Attendance, ,, group, tasks...</td>\n",
       "      <td>[Clustering, ,, matrix, for, patterns, ,, AI, ...</td>\n",
       "      <td>[use, the, data, and, build, representative, p...</td>\n",
       "      <td>[test, within, the, existing, network, to, see...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[data, analysis]</td>\n",
       "      <td>[to, collect, data]</td>\n",
       "      <td>[To, a, great, deal, ., can, help, process, la...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2/13/25 0:09</td>\n",
       "      <td>2/13/25 0:18</td>\n",
       "      <td>193.166.117.7</td>\n",
       "      <td>551</td>\n",
       "      <td>R_824J8ZB9ZGfOu8t</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1,2,4</td>\n",
       "      <td>4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[-]</td>\n",
       "      <td>[[, ', -, ', ]]</td>\n",
       "      <td>[[, ', -, ', ]]</td>\n",
       "      <td>[-]</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:55:35</td>\n",
       "      <td>2025-02-13 1:19:16</td>\n",
       "      <td>193.166.117.7</td>\n",
       "      <td>1421</td>\n",
       "      <td>R_2KPeTVjwHtRdvU7</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-age, ,, jobs, ,, demographic, data, ,, how, ...</td>\n",
       "      <td>[-clustering, students, according, to, their, ...</td>\n",
       "      <td>[-making, descriptive, profile, posters, for, ...</td>\n",
       "      <td>[-personas, should, not, become, static, but, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[To, make, posters, of, the, persona, so, that...</td>\n",
       "      <td>[For, analysis, ?, I, do, n't, know, what, is,...</td>\n",
       "      <td>[To, develop, personas, further, when, new, in...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  start_date_pre  end_date_pre  ip_address_pre  duration_sec_pre  \\\n",
       "0   2/13/25 0:10  2/13/25 0:16  193.166.113.18               372   \n",
       "1   2/13/25 0:09  2/13/25 0:16  193.166.113.32               409   \n",
       "2   2/13/25 0:09  2/13/25 0:17  193.166.113.31               462   \n",
       "3   2/13/25 0:11  2/13/25 0:17  193.166.113.21               355   \n",
       "4   2/13/25 0:09  2/13/25 0:18   193.166.117.7               551   \n",
       "\n",
       "     response_id_pre  LocationLatitude_pre  LocationLongitude_pre  \\\n",
       "0  R_8GBoA0i1k6yogS1               63.1198                21.6798   \n",
       "1  R_8QG2UV0Zv0fKVB5               63.1198                21.6798   \n",
       "2  R_8tHW5RDZd4yBkcx               63.1198                21.6798   \n",
       "3  R_8462bTis2cTNcii               63.1198                21.6798   \n",
       "4  R_824J8ZB9ZGfOu8t               63.1198                21.6798   \n",
       "\n",
       "   DistributionChannel_pre  UserLanguage_pre  participant_id  age_pre  \\\n",
       "0                        0                 0               1       60   \n",
       "1                        0                 0              23       41   \n",
       "2                        0                 0              22       45   \n",
       "3                        0                 0              19       39   \n",
       "4                        0                 0              15       38   \n",
       "\n",
       "   gender_pre  occupation_pre  teaching_marketing_pre teaching_experience_pre  \\\n",
       "0           1               1                       1                      20   \n",
       "1           1               2                       2                       0   \n",
       "2           1               2                       1                       3   \n",
       "3           1               2                       1                       1   \n",
       "4           2               2                       2                       0   \n",
       "\n",
       "  learning_style_pre learning_format_pre interaction_preference_pre  \\\n",
       "0                  3                 3,4                        3,4   \n",
       "1              1,3,4                 2,4                        3,4   \n",
       "2                1,3                 3,4                        3,4   \n",
       "3            1,2,3,4                 3,4                        3,4   \n",
       "4              1,2,4                   4                        3,4   \n",
       "\n",
       "  trusted_learning_method_pre  ai_familiarity_pre  ddp_familiarity_pre  \\\n",
       "0                       2,3,4                  12                   10   \n",
       "1                       2,3,4                  10                    1   \n",
       "2                       2,3,4                  11                   10   \n",
       "3                       2,3,4                  12                    1   \n",
       "4                           2                  12                    1   \n",
       "\n",
       "  data_sources_pre  persona_definition_pre  interactive_persona_pre  \\\n",
       "0                4                       1                        2   \n",
       "1                4                       1                        2   \n",
       "2              1,3                       1                        5   \n",
       "3                4                       1                        5   \n",
       "4                5                       5                        5   \n",
       "\n",
       "   data_driven_persona_pre  dynamic_persona_pre  \\\n",
       "0                        2                    1   \n",
       "1                        2                    1   \n",
       "2                        2                    5   \n",
       "3                        2                    5   \n",
       "4                        5                    5   \n",
       "\n",
       "                                   tableau_usage_pre  \\\n",
       "0                                 [I, do, not, know]   \n",
       "1  [I, do, not, know, these, tools, ,, so, I, can...   \n",
       "2                              [I, do, n't, know, .]   \n",
       "3                                              [nan]   \n",
       "4                                                [-]   \n",
       "\n",
       "                                       api_usage_pre  \\\n",
       "0  [[, ', I, ', ,, ', do, ', ,, ', not, ', ,, ', ...   \n",
       "1  [[, ', I, ', ,, ', have, ', ,, ', no, ', ,, ',...   \n",
       "2  [[, ', I, ', ,, ', do, ', ,, \", n't, \", ,, ', ...   \n",
       "3                                  [[, ', nan, ', ]]   \n",
       "4                                    [[, ', -, ', ]]   \n",
       "\n",
       "                                  ml_application_pre persona_explanation_pre  \\\n",
       "0  [[, ', I, ', ,, ', do, ', ,, ', not, ', ,, ', ...      [I, do, not, know]   \n",
       "1  [[, ', I, ', ,, ', have, ', ,, ', no, ', ,, ',...  [I, have, no, clue, .]   \n",
       "2  [[, ', I, ', ,, ', do, ', ,, \", n't, \", ,, ', ...   [I, do, n't, know, .]   \n",
       "3                                  [[, ', nan, ', ]]                   [nan]   \n",
       "4                                    [[, ', -, ', ]]                     [-]   \n",
       "\n",
       "   confirmation_pre      start_date_post        end_date_post ip_address_post  \\\n",
       "0                 0   2025-02-13 0:56:11   2025-02-13 1:18:59  193.166.113.18   \n",
       "1                 0   2025-02-13 0:56:02   2025-02-13 1:13:21  193.166.113.32   \n",
       "2                 0  2025-02-13 10:06:34  2025-02-13 10:19:38  193.166.113.31   \n",
       "3                 0   2025-02-13 0:55:51   2025-02-13 1:11:01  193.166.113.21   \n",
       "4                 0   2025-02-13 0:55:35   2025-02-13 1:19:16   193.166.117.7   \n",
       "\n",
       "   duration_sec_post   response_id_post  LocationLatitude_post  \\\n",
       "0               1368  R_8QL21bpDcdbdtJ4                63.1198   \n",
       "1               1039  R_8QrAaCxxXwbpxvL                63.1198   \n",
       "2                784  R_2kRKZFGiWHUQidw                63.1198   \n",
       "3                909  R_2M3diy3SR17ual3                63.1198   \n",
       "4               1421  R_2KPeTVjwHtRdvU7                63.1198   \n",
       "\n",
       "   LocationLongitude_post  DistributionChannel_post  UserLanguage_post  \\\n",
       "0                 21.6798                         0                  0   \n",
       "1                 21.6798                         0                  0   \n",
       "2                 21.6798                         0                  0   \n",
       "3                 21.6798                         0                  0   \n",
       "4                 21.6798                         0                  0   \n",
       "\n",
       "                    data_collection_explanation_post  \\\n",
       "0  [to, collect, studens, ', demographic, data, ,...   \n",
       "1  [User, analytics, :, Google, analytics, ,, CRM...   \n",
       "2  [I, need, quantitative, data, ,, which, can, b...   \n",
       "3  [Data, sources, :, Attendance, ,, group, tasks...   \n",
       "4  [-age, ,, jobs, ,, demographic, data, ,, how, ...   \n",
       "\n",
       "                      data_analysis_explanation_post  \\\n",
       "0  [First, the, data, must, be, cleaned, ., to, a...   \n",
       "1  [Segmentation, can, be, performed, so, that, e...   \n",
       "2  [Statistical, analyses, are, required, ., Thes...   \n",
       "3  [Clustering, ,, matrix, for, patterns, ,, AI, ...   \n",
       "4  [-clustering, students, according, to, their, ...   \n",
       "\n",
       "                   persona_building_explanation_post  \\\n",
       "0  [After, clustering, and, combining, ,, the, pe...   \n",
       "1  [One, way, to, create, student, personas, it, ...   \n",
       "2  [In, this, stage, ,, crafting, the, persona, p...   \n",
       "3  [use, the, data, and, build, representative, p...   \n",
       "4  [-making, descriptive, profile, posters, for, ...   \n",
       "\n",
       "                         evaluation_explanation_post  ai_familiarity_post  \\\n",
       "0  [The, evaluation, can, be, done, e.g., ,, with...                    4   \n",
       "1  [1, ), I, have, not, been, teaching, marketing...                    2   \n",
       "2  [This, is, a, crucial, step, in, the, persona,...                    3   \n",
       "3  [test, within, the, existing, network, to, see...                    4   \n",
       "4  [-personas, should, not, become, static, but, ...                    4   \n",
       "\n",
       "   ddp_familiarity_post data_sources_post  persona_definition_post  \\\n",
       "0                     4  All of the above                        0   \n",
       "1                     2  All of the above                        0   \n",
       "2                     4  All of the above                        0   \n",
       "3                     5  All of the above                        0   \n",
       "4                     2  All of the above                        0   \n",
       "\n",
       "   interactive_persona_post  data_driven_persona_post  dynamic_persona_post  \\\n",
       "0                         1                         0                     0   \n",
       "1                         1                         0                     0   \n",
       "2                         2                         0                     1   \n",
       "3                         1                         0                     0   \n",
       "4                         1                         0                     0   \n",
       "\n",
       "                                    tools_usage_post  \\\n",
       "0  [To, communicate, the, personas, to, the, user...   \n",
       "1  [To, illustrate, (, and, update, ), static, pe...   \n",
       "2                   [To, visualise, the, persona, .]   \n",
       "3                                   [data, analysis]   \n",
       "4  [To, make, posters, of, the, persona, so, that...   \n",
       "\n",
       "                                      api_usage_post  \\\n",
       "0  [Because, relevant, data, can, be, obtained, f...   \n",
       "1              [I, have, no, clue, what, is, API, .]   \n",
       "2       [I, do, nt, know, what, API, stands, for, .]   \n",
       "3                                [to, collect, data]   \n",
       "4  [For, analysis, ?, I, do, n't, know, what, is,...   \n",
       "\n",
       "                                 ml_application_post  \\\n",
       "0  [They, can, be, used, e.g., ,, in, clustering,...   \n",
       "1                 [To, create, dynamic, personas, .]   \n",
       "2                              [I, do, n't, know, .]   \n",
       "3  [To, a, great, deal, ., can, help, process, la...   \n",
       "4  [To, develop, personas, further, when, new, in...   \n",
       "\n",
       "   engagement_experience_post  interaction_quality_post  \\\n",
       "0                           5                         1   \n",
       "1                           4                         2   \n",
       "2                           3                         4   \n",
       "3                           6                         6   \n",
       "4                           4                         2   \n",
       "\n",
       "   communication_clarity_post  trustworthiness_post  emotional_response_post  \\\n",
       "0                           7                     2                        2   \n",
       "1                           4                     4                        2   \n",
       "2                           6                     6                        1   \n",
       "3                           6                     4                        2   \n",
       "4                           3                     3                        1   \n",
       "\n",
       "   naturalness_post  effectiveness_post  comfort_level_post  \\\n",
       "0                 1                   6                   6   \n",
       "1                 4                   4                   2   \n",
       "2                 4                   5                   3   \n",
       "3                 3                   6                   1   \n",
       "4                 4                   3                   6   \n",
       "\n",
       "   personalization_post  mental_effort_post  \n",
       "0                     1                   4  \n",
       "1                     2                   6  \n",
       "2                     4                   5  \n",
       "3                     4                   4  \n",
       "4                     1                   6  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the cleaned dataset\n",
    "# Ensure all columns are displayed\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
