{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scipy scikit-learn spacy gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 6.0 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy English model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 01.1_df_word_token_stopwords_lemmatize.csv (Rows: 26, Columns: 82)\n",
      "Loaded: 01.2_df_subword_token_stopwords_lemmatize.csv (Rows: 26, Columns: 82)\n",
      "Loaded: 01.3_df_sentence_token_stopwords_lemmatize.csv (Rows: 26, Columns: 82)\n",
      "Loaded: 01.4_df_bert_token_stopwords_lemmatize.csv (Rows: 26, Columns: 82)\n",
      "Loaded: 01.5_df_tiktoken_token_stopwords_lemmatize.csv (Rows: 26, Columns: 82)\n",
      "Loaded: 01.6_df_whitespace_token_stopwords_lemmatize.csv (Rows: 26, Columns: 82)\n",
      "\n",
      " Successfully loaded 6 out of 6 files.\n"
     ]
    }
   ],
   "source": [
    "# List of survey files to process\n",
    "survey_files = [\n",
    "    \"01.1_df_word_token_stopwords_lemmatize.csv\",\n",
    "    \"01.2_df_subword_token_stopwords_lemmatize.csv\",\n",
    "    \"01.3_df_sentence_token_stopwords_lemmatize.csv\",  \n",
    "    \"01.4_df_bert_token_stopwords_lemmatize.csv\", \n",
    "    \"01.5_df_tiktoken_token_stopwords_lemmatize.csv\",  \n",
    "    \"01.6_df_whitespace_token_stopwords_lemmatize.csv\"\n",
    "]\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through each file and check if it exists\n",
    "for file in survey_files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes[file] = df  # Store DataFrame in dictionary\n",
    "            print(f\"Loaded: {file} (Rows: {df.shape[0]}, Columns: {df.shape[1]})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n Successfully loaded {len(dataframes)} out of {len(survey_files)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on answers length\n",
    "# we splitted them in two lists\n",
    "short_text_columns = [\"api_usage_pre\", \"tableau_usage_pre\", \"ml_application_pre\", \"persona_explanation_pre\", \"tools_usage_post\", \"api_usage_post\", \"ml_application_post\", ]\n",
    "long_text_columns = [\"data_collection_explanation_post\", \"data_analysis_explanation_post\", \"persona_building_explanation_post\", \"evaluation_explanation_post\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe model (50-dimensional vectors)\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store vectorized data for each survey\n",
    "vectorized_surveys = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_with_glove(sentence, model, vector_size=50):\n",
    "    \"\"\"\n",
    "    Convert a tokenized sentence into a GloVe word embedding vector.\n",
    "    If a word is not in GloVe, it is ignored.\n",
    "    \"\"\"\n",
    "    words = [token.text for token in nlp(sentence) if token.text in model]\n",
    "    if not words:\n",
    "        return np.zeros(vector_size)  # Return zero vector if no known words are found\n",
    "    return np.mean([model[word] for word in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_survey(file_name):\n",
    "    \"\"\"\n",
    "    Vectorizes a single survey dataset.\n",
    "    - Uses TF-IDF for short responses\n",
    "    - Uses GloVe embeddings for open-ended responses\n",
    "    - Combines both into a single feature matrix\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_name)\n",
    "        print(f\"Processing survey: {file_name}\")\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        short_response_vectors = tfidf_vectorizer.fit_transform(df[short_text_columns].fillna(\"\").agg(\" \".join, axis=1))\n",
    "\n",
    "        df[\"glove_vector\"] = df[long_text_columns].fillna(\"\").agg(\" \".join, axis=1).apply(lambda x: vectorize_sentence_with_glove(x, glove_model, 50))\n",
    "\n",
    "        glove_vectors = np.vstack(df[\"glove_vector\"])\n",
    "\n",
    "        final_feature_matrix = hstack([short_response_vectors, glove_vectors])\n",
    "\n",
    "        print(f\"Vectorization complete for: {file_name}, Shape: {final_feature_matrix.shape}\")\n",
    "\n",
    "        return final_feature_matrix\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing survey: 01.1_df_word_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.1_df_word_token_stopwords_lemmatize.csv, Shape: (26, 339)\n",
      "Processing survey: 01.2_df_subword_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.2_df_subword_token_stopwords_lemmatize.csv, Shape: (26, 579)\n",
      "Processing survey: 01.3_df_sentence_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.3_df_sentence_token_stopwords_lemmatize.csv, Shape: (26, 348)\n",
      "Processing survey: 01.4_df_bert_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.4_df_bert_token_stopwords_lemmatize.csv, Shape: (26, 360)\n",
      "Processing survey: 01.5_df_tiktoken_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.5_df_tiktoken_token_stopwords_lemmatize.csv, Shape: (26, 725)\n",
      "Processing survey: 01.6_df_whitespace_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.6_df_whitespace_token_stopwords_lemmatize.csv, Shape: (26, 341)\n"
     ]
    }
   ],
   "source": [
    "# Process each survey file\n",
    "for survey_file in survey_files:\n",
    "    vectorized_surveys[survey_file] = process_survey(survey_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized data saved: 01.1_df_word_token_stopwords_lemmatize_vectorized.npz\n",
      "Vectorized data saved: 01.2_df_subword_token_stopwords_lemmatize_vectorized.npz\n",
      "Vectorized data saved: 01.3_df_sentence_token_stopwords_lemmatize_vectorized.npz\n",
      "Vectorized data saved: 01.4_df_bert_token_stopwords_lemmatize_vectorized.npz\n",
      "Vectorized data saved: 01.5_df_tiktoken_token_stopwords_lemmatize_vectorized.npz\n",
      "Vectorized data saved: 01.6_df_whitespace_token_stopwords_lemmatize_vectorized.npz\n"
     ]
    }
   ],
   "source": [
    "# Save vectorized data for each survey\n",
    "for survey_name, feature_matrix in vectorized_surveys.items():\n",
    "    if feature_matrix is not None:\n",
    "        output_file = survey_name.replace(\".csv\", \"_vectorized.npz\")\n",
    "        np.savez_compressed(output_file, feature_matrix=feature_matrix)\n",
    "        print(f\"Vectorized data saved: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
