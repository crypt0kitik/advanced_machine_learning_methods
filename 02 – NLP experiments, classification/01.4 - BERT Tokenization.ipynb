{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "import spacy\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def bert_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text using BERT's WordPiece tokenizer.\n",
    "    :param text: str, input text\n",
    "    :return: list, tokenized subwords\n",
    "    \"\"\"\n",
    "    return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"all_numeric_survey.csv\" \n",
    "text_columns = [\"tableau_usage_pre\", \"api_usage_pre\", \"ml_application_pre\",\n",
    "                    \"persona_explanation_pre\", \"api_usage_pre\", \"ml_application_pre\",\n",
    "                    \"data_collection_explanation_post\", \"data_analysis_explanation_post\", \"persona_building_explanation_post\",\n",
    "                    \"evaluation_explanation_post\", \"tools_usage_post\", \"api_usage_post\",\n",
    "                    \"ml_application_post\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Processed dataset saved as dataset_bert_token.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(file_path)  # Load CSV file\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "\n",
    "    # Apply lemmatization to each text column\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:  # Ensure column exists before processing\n",
    "            df[col] = df[col].astype(str).apply(bert_tokenize)\n",
    "\n",
    "    # Save the processed dataset\n",
    "    output_file = \"dataset_bert_token.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed dataset saved as {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date_pre</th>\n",
       "      <th>end_date_pre</th>\n",
       "      <th>ip_address_pre</th>\n",
       "      <th>duration_sec_pre</th>\n",
       "      <th>response_id_pre</th>\n",
       "      <th>LocationLatitude_pre</th>\n",
       "      <th>LocationLongitude_pre</th>\n",
       "      <th>DistributionChannel_pre</th>\n",
       "      <th>UserLanguage_pre</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>age_pre</th>\n",
       "      <th>gender_pre</th>\n",
       "      <th>occupation_pre</th>\n",
       "      <th>teaching_marketing_pre</th>\n",
       "      <th>teaching_experience_pre</th>\n",
       "      <th>learning_style_pre</th>\n",
       "      <th>learning_format_pre</th>\n",
       "      <th>interaction_preference_pre</th>\n",
       "      <th>trusted_learning_method_pre</th>\n",
       "      <th>ai_familiarity_pre</th>\n",
       "      <th>ddp_familiarity_pre</th>\n",
       "      <th>data_sources_pre</th>\n",
       "      <th>persona_definition_pre</th>\n",
       "      <th>interactive_persona_pre</th>\n",
       "      <th>data_driven_persona_pre</th>\n",
       "      <th>dynamic_persona_pre</th>\n",
       "      <th>tableau_usage_pre</th>\n",
       "      <th>api_usage_pre</th>\n",
       "      <th>ml_application_pre</th>\n",
       "      <th>persona_explanation_pre</th>\n",
       "      <th>confirmation_pre</th>\n",
       "      <th>start_date_post</th>\n",
       "      <th>end_date_post</th>\n",
       "      <th>ip_address_post</th>\n",
       "      <th>duration_sec_post</th>\n",
       "      <th>response_id_post</th>\n",
       "      <th>LocationLatitude_post</th>\n",
       "      <th>LocationLongitude_post</th>\n",
       "      <th>DistributionChannel_post</th>\n",
       "      <th>UserLanguage_post</th>\n",
       "      <th>data_collection_explanation_post</th>\n",
       "      <th>data_analysis_explanation_post</th>\n",
       "      <th>persona_building_explanation_post</th>\n",
       "      <th>evaluation_explanation_post</th>\n",
       "      <th>ai_familiarity_post</th>\n",
       "      <th>ddp_familiarity_post</th>\n",
       "      <th>data_sources_post</th>\n",
       "      <th>persona_definition_post</th>\n",
       "      <th>interactive_persona_post</th>\n",
       "      <th>data_driven_persona_post</th>\n",
       "      <th>dynamic_persona_post</th>\n",
       "      <th>tools_usage_post</th>\n",
       "      <th>api_usage_post</th>\n",
       "      <th>ml_application_post</th>\n",
       "      <th>engagement_experience_post</th>\n",
       "      <th>interaction_quality_post</th>\n",
       "      <th>communication_clarity_post</th>\n",
       "      <th>trustworthiness_post</th>\n",
       "      <th>emotional_response_post</th>\n",
       "      <th>naturalness_post</th>\n",
       "      <th>effectiveness_post</th>\n",
       "      <th>comfort_level_post</th>\n",
       "      <th>personalization_post</th>\n",
       "      <th>mental_effort_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2/13/25 0:10</td>\n",
       "      <td>2/13/25 0:16</td>\n",
       "      <td>193.166.113.18</td>\n",
       "      <td>372</td>\n",
       "      <td>R_8GBoA0i1k6yogS1</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, do, not, know]</td>\n",
       "      <td>[[, ', i, ', ,, ', do, ', ,, ', not, ', ,, ', ...</td>\n",
       "      <td>[[, ', i, ', ,, ', do, ', ,, ', not, ', ,, ', ...</td>\n",
       "      <td>[i, do, not, know]</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:56:11</td>\n",
       "      <td>2025-02-13 1:18:59</td>\n",
       "      <td>193.166.113.18</td>\n",
       "      <td>1368</td>\n",
       "      <td>R_8QL21bpDcdbdtJ4</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[to, collect, stud, ##ens, ', demographic, dat...</td>\n",
       "      <td>[first, the, data, must, be, cleaned, ., to, a...</td>\n",
       "      <td>[after, cluster, ##ing, and, combining, ,, the...</td>\n",
       "      <td>[the, evaluation, can, be, done, e, ., g, ., ,...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[to, communicate, the, persona, ##s, to, the, ...</td>\n",
       "      <td>[because, relevant, data, can, be, obtained, f...</td>\n",
       "      <td>[they, can, be, used, e, ., g, ., ,, in, clust...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/13/25 0:09</td>\n",
       "      <td>2/13/25 0:16</td>\n",
       "      <td>193.166.113.32</td>\n",
       "      <td>409</td>\n",
       "      <td>R_8QG2UV0Zv0fKVB5</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1,3,4</td>\n",
       "      <td>2,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, do, not, know, these, tools, ,, so, i, can...</td>\n",
       "      <td>[[, ', i, ', ,, ', have, ', ,, ', no, ', ,, ',...</td>\n",
       "      <td>[[, ', i, ', ,, ', have, ', ,, ', no, ', ,, ',...</td>\n",
       "      <td>[i, have, no, clue, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:56:02</td>\n",
       "      <td>2025-02-13 1:13:21</td>\n",
       "      <td>193.166.113.32</td>\n",
       "      <td>1039</td>\n",
       "      <td>R_8QrAaCxxXwbpxvL</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[user, analytics, :, google, analytics, ,, cr,...</td>\n",
       "      <td>[segment, ##ation, can, be, performed, so, tha...</td>\n",
       "      <td>[one, way, to, create, student, persona, ##s, ...</td>\n",
       "      <td>[1, ), i, have, not, been, teaching, marketing...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[to, illustrate, (, and, update, ), static, pe...</td>\n",
       "      <td>[i, have, no, clue, what, is, api, .]</td>\n",
       "      <td>[to, create, dynamic, persona, ##s, .]</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2/13/25 0:09</td>\n",
       "      <td>2/13/25 0:17</td>\n",
       "      <td>193.166.113.31</td>\n",
       "      <td>462</td>\n",
       "      <td>R_8tHW5RDZd4yBkcx</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1,3</td>\n",
       "      <td>3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1,3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[i, don, ', t, know, .]</td>\n",
       "      <td>[[, ', i, ', ,, ', don, ', ,, \", ', \", ,, ', t...</td>\n",
       "      <td>[[, ', i, ', ,, ', don, ', ,, \", ', \", ,, ', t...</td>\n",
       "      <td>[i, don, ', t, know, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 10:06:34</td>\n",
       "      <td>2025-02-13 10:19:38</td>\n",
       "      <td>193.166.113.31</td>\n",
       "      <td>784</td>\n",
       "      <td>R_2kRKZFGiWHUQidw</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, need, quantitative, data, ,, which, can, b...</td>\n",
       "      <td>[statistical, analyses, are, required, ., thes...</td>\n",
       "      <td>[in, this, stage, ,, craft, ##ing, the, person...</td>\n",
       "      <td>[this, is, a, crucial, step, in, the, persona,...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[to, visual, ##ise, the, persona, .]</td>\n",
       "      <td>[i, don, ##t, know, what, api, stands, for, .]</td>\n",
       "      <td>[i, don, ', t, know, .]</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2/13/25 0:11</td>\n",
       "      <td>2/13/25 0:17</td>\n",
       "      <td>193.166.113.21</td>\n",
       "      <td>355</td>\n",
       "      <td>R_8462bTis2cTNcii</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1,2,3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[, ', nan, ', ]]</td>\n",
       "      <td>[[, ', nan, ', ]]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:55:51</td>\n",
       "      <td>2025-02-13 1:11:01</td>\n",
       "      <td>193.166.113.21</td>\n",
       "      <td>909</td>\n",
       "      <td>R_2M3diy3SR17ual3</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[data, sources, :, attendance, ,, group, tasks...</td>\n",
       "      <td>[cluster, ##ing, ,, matrix, for, patterns, ,, ...</td>\n",
       "      <td>[use, the, data, and, build, representative, p...</td>\n",
       "      <td>[test, within, the, existing, network, to, see...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[data, analysis]</td>\n",
       "      <td>[to, collect, data]</td>\n",
       "      <td>[to, a, great, deal, ., can, help, process, la...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2/13/25 0:09</td>\n",
       "      <td>2/13/25 0:18</td>\n",
       "      <td>193.166.117.7</td>\n",
       "      <td>551</td>\n",
       "      <td>R_824J8ZB9ZGfOu8t</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1,2,4</td>\n",
       "      <td>4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[-]</td>\n",
       "      <td>[[, ', -, ', ]]</td>\n",
       "      <td>[[, ', -, ', ]]</td>\n",
       "      <td>[-]</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:55:35</td>\n",
       "      <td>2025-02-13 1:19:16</td>\n",
       "      <td>193.166.117.7</td>\n",
       "      <td>1421</td>\n",
       "      <td>R_2KPeTVjwHtRdvU7</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-, age, ,, jobs, ,, demographic, data, ,, how...</td>\n",
       "      <td>[-, cluster, ##ing, students, according, to, t...</td>\n",
       "      <td>[-, making, descriptive, profile, posters, for...</td>\n",
       "      <td>[-, persona, ##s, should, not, become, static,...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[to, make, posters, of, the, persona, so, that...</td>\n",
       "      <td>[for, analysis, ?, i, don, ', t, know, what, i...</td>\n",
       "      <td>[to, develop, persona, ##s, further, when, new...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  start_date_pre  end_date_pre  ip_address_pre  duration_sec_pre  \\\n",
       "0   2/13/25 0:10  2/13/25 0:16  193.166.113.18               372   \n",
       "1   2/13/25 0:09  2/13/25 0:16  193.166.113.32               409   \n",
       "2   2/13/25 0:09  2/13/25 0:17  193.166.113.31               462   \n",
       "3   2/13/25 0:11  2/13/25 0:17  193.166.113.21               355   \n",
       "4   2/13/25 0:09  2/13/25 0:18   193.166.117.7               551   \n",
       "\n",
       "     response_id_pre  LocationLatitude_pre  LocationLongitude_pre  \\\n",
       "0  R_8GBoA0i1k6yogS1               63.1198                21.6798   \n",
       "1  R_8QG2UV0Zv0fKVB5               63.1198                21.6798   \n",
       "2  R_8tHW5RDZd4yBkcx               63.1198                21.6798   \n",
       "3  R_8462bTis2cTNcii               63.1198                21.6798   \n",
       "4  R_824J8ZB9ZGfOu8t               63.1198                21.6798   \n",
       "\n",
       "   DistributionChannel_pre  UserLanguage_pre  participant_id  age_pre  \\\n",
       "0                        0                 0               1       60   \n",
       "1                        0                 0              23       41   \n",
       "2                        0                 0              22       45   \n",
       "3                        0                 0              19       39   \n",
       "4                        0                 0              15       38   \n",
       "\n",
       "   gender_pre  occupation_pre  teaching_marketing_pre teaching_experience_pre  \\\n",
       "0           1               1                       1                      20   \n",
       "1           1               2                       2                       0   \n",
       "2           1               2                       1                       3   \n",
       "3           1               2                       1                       1   \n",
       "4           2               2                       2                       0   \n",
       "\n",
       "  learning_style_pre learning_format_pre interaction_preference_pre  \\\n",
       "0                  3                 3,4                        3,4   \n",
       "1              1,3,4                 2,4                        3,4   \n",
       "2                1,3                 3,4                        3,4   \n",
       "3            1,2,3,4                 3,4                        3,4   \n",
       "4              1,2,4                   4                        3,4   \n",
       "\n",
       "  trusted_learning_method_pre  ai_familiarity_pre  ddp_familiarity_pre  \\\n",
       "0                       2,3,4                  12                   10   \n",
       "1                       2,3,4                  10                    1   \n",
       "2                       2,3,4                  11                   10   \n",
       "3                       2,3,4                  12                    1   \n",
       "4                           2                  12                    1   \n",
       "\n",
       "  data_sources_pre  persona_definition_pre  interactive_persona_pre  \\\n",
       "0                4                       1                        2   \n",
       "1                4                       1                        2   \n",
       "2              1,3                       1                        5   \n",
       "3                4                       1                        5   \n",
       "4                5                       5                        5   \n",
       "\n",
       "   data_driven_persona_pre  dynamic_persona_pre  \\\n",
       "0                        2                    1   \n",
       "1                        2                    1   \n",
       "2                        2                    5   \n",
       "3                        2                    5   \n",
       "4                        5                    5   \n",
       "\n",
       "                                   tableau_usage_pre  \\\n",
       "0                                 [i, do, not, know]   \n",
       "1  [i, do, not, know, these, tools, ,, so, i, can...   \n",
       "2                            [i, don, ', t, know, .]   \n",
       "3                                              [nan]   \n",
       "4                                                [-]   \n",
       "\n",
       "                                       api_usage_pre  \\\n",
       "0  [[, ', i, ', ,, ', do, ', ,, ', not, ', ,, ', ...   \n",
       "1  [[, ', i, ', ,, ', have, ', ,, ', no, ', ,, ',...   \n",
       "2  [[, ', i, ', ,, ', don, ', ,, \", ', \", ,, ', t...   \n",
       "3                                  [[, ', nan, ', ]]   \n",
       "4                                    [[, ', -, ', ]]   \n",
       "\n",
       "                                  ml_application_pre  persona_explanation_pre  \\\n",
       "0  [[, ', i, ', ,, ', do, ', ,, ', not, ', ,, ', ...       [i, do, not, know]   \n",
       "1  [[, ', i, ', ,, ', have, ', ,, ', no, ', ,, ',...   [i, have, no, clue, .]   \n",
       "2  [[, ', i, ', ,, ', don, ', ,, \", ', \", ,, ', t...  [i, don, ', t, know, .]   \n",
       "3                                  [[, ', nan, ', ]]                    [nan]   \n",
       "4                                    [[, ', -, ', ]]                      [-]   \n",
       "\n",
       "   confirmation_pre      start_date_post        end_date_post ip_address_post  \\\n",
       "0                 0   2025-02-13 0:56:11   2025-02-13 1:18:59  193.166.113.18   \n",
       "1                 0   2025-02-13 0:56:02   2025-02-13 1:13:21  193.166.113.32   \n",
       "2                 0  2025-02-13 10:06:34  2025-02-13 10:19:38  193.166.113.31   \n",
       "3                 0   2025-02-13 0:55:51   2025-02-13 1:11:01  193.166.113.21   \n",
       "4                 0   2025-02-13 0:55:35   2025-02-13 1:19:16   193.166.117.7   \n",
       "\n",
       "   duration_sec_post   response_id_post  LocationLatitude_post  \\\n",
       "0               1368  R_8QL21bpDcdbdtJ4                63.1198   \n",
       "1               1039  R_8QrAaCxxXwbpxvL                63.1198   \n",
       "2                784  R_2kRKZFGiWHUQidw                63.1198   \n",
       "3                909  R_2M3diy3SR17ual3                63.1198   \n",
       "4               1421  R_2KPeTVjwHtRdvU7                63.1198   \n",
       "\n",
       "   LocationLongitude_post  DistributionChannel_post  UserLanguage_post  \\\n",
       "0                 21.6798                         0                  0   \n",
       "1                 21.6798                         0                  0   \n",
       "2                 21.6798                         0                  0   \n",
       "3                 21.6798                         0                  0   \n",
       "4                 21.6798                         0                  0   \n",
       "\n",
       "                    data_collection_explanation_post  \\\n",
       "0  [to, collect, stud, ##ens, ', demographic, dat...   \n",
       "1  [user, analytics, :, google, analytics, ,, cr,...   \n",
       "2  [i, need, quantitative, data, ,, which, can, b...   \n",
       "3  [data, sources, :, attendance, ,, group, tasks...   \n",
       "4  [-, age, ,, jobs, ,, demographic, data, ,, how...   \n",
       "\n",
       "                      data_analysis_explanation_post  \\\n",
       "0  [first, the, data, must, be, cleaned, ., to, a...   \n",
       "1  [segment, ##ation, can, be, performed, so, tha...   \n",
       "2  [statistical, analyses, are, required, ., thes...   \n",
       "3  [cluster, ##ing, ,, matrix, for, patterns, ,, ...   \n",
       "4  [-, cluster, ##ing, students, according, to, t...   \n",
       "\n",
       "                   persona_building_explanation_post  \\\n",
       "0  [after, cluster, ##ing, and, combining, ,, the...   \n",
       "1  [one, way, to, create, student, persona, ##s, ...   \n",
       "2  [in, this, stage, ,, craft, ##ing, the, person...   \n",
       "3  [use, the, data, and, build, representative, p...   \n",
       "4  [-, making, descriptive, profile, posters, for...   \n",
       "\n",
       "                         evaluation_explanation_post  ai_familiarity_post  \\\n",
       "0  [the, evaluation, can, be, done, e, ., g, ., ,...                    4   \n",
       "1  [1, ), i, have, not, been, teaching, marketing...                    2   \n",
       "2  [this, is, a, crucial, step, in, the, persona,...                    3   \n",
       "3  [test, within, the, existing, network, to, see...                    4   \n",
       "4  [-, persona, ##s, should, not, become, static,...                    4   \n",
       "\n",
       "   ddp_familiarity_post data_sources_post  persona_definition_post  \\\n",
       "0                     4  All of the above                        0   \n",
       "1                     2  All of the above                        0   \n",
       "2                     4  All of the above                        0   \n",
       "3                     5  All of the above                        0   \n",
       "4                     2  All of the above                        0   \n",
       "\n",
       "   interactive_persona_post  data_driven_persona_post  dynamic_persona_post  \\\n",
       "0                         1                         0                     0   \n",
       "1                         1                         0                     0   \n",
       "2                         2                         0                     1   \n",
       "3                         1                         0                     0   \n",
       "4                         1                         0                     0   \n",
       "\n",
       "                                    tools_usage_post  \\\n",
       "0  [to, communicate, the, persona, ##s, to, the, ...   \n",
       "1  [to, illustrate, (, and, update, ), static, pe...   \n",
       "2               [to, visual, ##ise, the, persona, .]   \n",
       "3                                   [data, analysis]   \n",
       "4  [to, make, posters, of, the, persona, so, that...   \n",
       "\n",
       "                                      api_usage_post  \\\n",
       "0  [because, relevant, data, can, be, obtained, f...   \n",
       "1              [i, have, no, clue, what, is, api, .]   \n",
       "2     [i, don, ##t, know, what, api, stands, for, .]   \n",
       "3                                [to, collect, data]   \n",
       "4  [for, analysis, ?, i, don, ', t, know, what, i...   \n",
       "\n",
       "                                 ml_application_post  \\\n",
       "0  [they, can, be, used, e, ., g, ., ,, in, clust...   \n",
       "1             [to, create, dynamic, persona, ##s, .]   \n",
       "2                            [i, don, ', t, know, .]   \n",
       "3  [to, a, great, deal, ., can, help, process, la...   \n",
       "4  [to, develop, persona, ##s, further, when, new...   \n",
       "\n",
       "   engagement_experience_post  interaction_quality_post  \\\n",
       "0                           5                         1   \n",
       "1                           4                         2   \n",
       "2                           3                         4   \n",
       "3                           6                         6   \n",
       "4                           4                         2   \n",
       "\n",
       "   communication_clarity_post  trustworthiness_post  emotional_response_post  \\\n",
       "0                           7                     2                        2   \n",
       "1                           4                     4                        2   \n",
       "2                           6                     6                        1   \n",
       "3                           6                     4                        2   \n",
       "4                           3                     3                        1   \n",
       "\n",
       "   naturalness_post  effectiveness_post  comfort_level_post  \\\n",
       "0                 1                   6                   6   \n",
       "1                 4                   4                   2   \n",
       "2                 4                   5                   3   \n",
       "3                 3                   6                   1   \n",
       "4                 4                   3                   6   \n",
       "\n",
       "   personalization_post  mental_effort_post  \n",
       "0                     1                   4  \n",
       "1                     2                   6  \n",
       "2                     4                   5  \n",
       "3                     4                   4  \n",
       "4                     1                   6  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the cleaned dataset\n",
    "# Ensure all columns are displayed\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
