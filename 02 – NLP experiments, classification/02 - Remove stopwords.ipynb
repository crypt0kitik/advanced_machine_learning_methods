{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "\n",
    "# I had an issue with NLTK\n",
    "# that is I opted for Spacy\n",
    "import spacy\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ensure spaCy model is downloaded\n",
    "def download_spacy_model():\n",
    "    try:\n",
    "        spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(\"Downloading 'en_core_web_sm' model...\")\n",
    "        subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
    "        print(\"Download complete!\")\n",
    "\n",
    "# Ensure spaCy model is available\n",
    "download_spacy_model()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Function to remove stopwords from the given text using spaCy.\n",
    "    :param text: str, input text\n",
    "    :return: str, text without stopwords\n",
    "    \"\"\"\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return \"\"\n",
    "    \n",
    "    doc = nlp(text)  # Process the text using spaCy\n",
    "    filtered_text = [token.text for token in doc if not token.is_stop]  # Remove stopwords\n",
    "    return \" \".join(filtered_text)  # Join words back into a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datasets = [\n",
    "    \"dataset_word_token.csv\",\n",
    "    \"dataset_subword_token.csv\",\n",
    "    \"dataset_sentence_token.csv\",\n",
    "    \"dataset_bert_token.csv\",\n",
    "    \"dataset_tiktoken_token.csv\",\n",
    "    \"dataset_whitespace_token.csv\"\n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dataset_word_token.csv loaded successfully!\n",
      "Processed dataset saved as dataset_word_token_stopwords.csv\n",
      "Dataset dataset_subword_token.csv loaded successfully!\n",
      "Processed dataset saved as dataset_subword_token_stopwords.csv\n",
      "Dataset dataset_sentence_token.csv loaded successfully!\n",
      "Processed dataset saved as dataset_sentence_token_stopwords.csv\n",
      "Dataset dataset_bert_token.csv loaded successfully!\n",
      "Processed dataset saved as dataset_bert_token_stopwords.csv\n",
      "Dataset dataset_tiktoken_token.csv loaded successfully!\n",
      "Processed dataset saved as dataset_tiktoken_token_stopwords.csv\n",
      "Dataset dataset_whitespace_token.csv loaded successfully!\n",
      "Processed dataset saved as dataset_whitespace_token_stopwords.csv\n"
     ]
    }
   ],
   "source": [
    "# Define stopword removal function (Make sure it's implemented)\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Function to remove stopwords from text (Assume it's implemented)\"\"\"\n",
    "    return text  # Placeholder, replace with actual implementation\n",
    "\n",
    "# List of columns to clean (Replace with actual column names)\n",
    "text_columns = [\n",
    "    \"tableau_usage_pre\", \"api_usage_pre\", \"ml_application_pre\",\n",
    "    \"persona_explanation_pre\", \"api_usage_pre\", \"ml_application_pre\",\n",
    "    \"data_collection_explanation_post\", \"data_analysis_explanation_post\",\n",
    "    \"persona_building_explanation_post\", \"evaluation_explanation_post\",\n",
    "    \"tools_usage_post\", \"api_usage_post\", \"ml_application_post\"\n",
    "]\n",
    "\n",
    "# Process each dataset\n",
    "for dataset in datasets:\n",
    "    try:\n",
    "        file_path = dataset  # Assign dataset name directly\n",
    "\n",
    "        # Load dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset {dataset} loaded successfully!\")\n",
    "\n",
    "        # Apply stopword removal to each column in the list\n",
    "        for col in text_columns:\n",
    "            if col in df.columns:  # Ensure column exists before processing\n",
    "                df[col] = df[col].astype(str).apply(remove_stopwords)\n",
    "\n",
    "        # Generate a unique name for the processed dataset\n",
    "        output_file = file_path.replace(\".csv\", \"_stopwords.csv\")\n",
    "\n",
    "        # Save the processed dataset (overwrite old data)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed dataset saved as {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset {dataset}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
