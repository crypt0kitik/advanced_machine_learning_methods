{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had an issue with NLTK\n",
    "# that is I opted for spacy\n",
    "import spacy\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the spaCy model if not already installed\n",
    "def download_spacy_model():\n",
    "    try:\n",
    "        spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(\"Downloading 'en_core_web_sm' model...\")\n",
    "        subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
    "        print(\"Download complete!\")\n",
    "\n",
    "# Ensure the spaCy model is available\n",
    "download_spacy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Function to lemmatize text using spaCy.\n",
    "    :param text: str, input text\n",
    "    :return: str, lemmatized text\n",
    "    \"\"\"\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return \"\"\n",
    "\n",
    "    doc = nlp(text)  # Process the text using spaCy\n",
    "    return \" \".join([token.lemma_ for token in doc])  # Apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datasets = [\n",
    "    \"dataset_word_token.csv\",\n",
    "    \"dataset_subword_token.csv\",\n",
    "    \"dataset_sentence_token.csv\",\n",
    "    \"dataset_bert_token.csv\",\n",
    "    \"dataset_tiktoken_token.csv\",\n",
    "    \"dataset_whitespace_token.csv\"\n",
    "] \n",
    "\n",
    "text_columns = [\"tableau_usage_pre\", \"api_usage_pre\", \"ml_application_pre\",\n",
    "                    \"persona_explanation_pre\", \"api_usage_pre\", \"ml_application_pre\",\n",
    "                    \"data_collection_explanation_post\", \"data_analysis_explanation_post\", \"persona_building_explanation_post\",\n",
    "                    \"evaluation_explanation_post\", \"tools_usage_post\", \"api_usage_post\",\n",
    "                    \"ml_application_post\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dataset_word_token.csv loaded successfully!\n",
      "Error loading dataset dataset_word_token.csv: name 'nlp' is not defined\n",
      "Dataset dataset_subword_token.csv loaded successfully!\n",
      "Error loading dataset dataset_subword_token.csv: name 'nlp' is not defined\n",
      "Dataset dataset_sentence_token.csv loaded successfully!\n",
      "Error loading dataset dataset_sentence_token.csv: name 'nlp' is not defined\n",
      "Dataset dataset_bert_token.csv loaded successfully!\n",
      "Error loading dataset dataset_bert_token.csv: name 'nlp' is not defined\n",
      "Dataset dataset_tiktoken_token.csv loaded successfully!\n",
      "Error loading dataset dataset_tiktoken_token.csv: name 'nlp' is not defined\n",
      "Dataset dataset_whitespace_token.csv loaded successfully!\n",
      "Error loading dataset dataset_whitespace_token.csv: name 'nlp' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Process each dataset\n",
    "for dataset in datasets:\n",
    "    try:\n",
    "        file_path = dataset  # Assign dataset name directly\n",
    "\n",
    "        # Load dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset {dataset} loaded successfully!\")\n",
    "\n",
    "        # Apply lemmatization to each column in the list\n",
    "        for col in text_columns:\n",
    "            if col in df.columns:  # Ensure column exists before processing\n",
    "                df[col] = df[col].astype(str).apply(lemmatize_text)\n",
    "\n",
    "        # Generate a unique name for the processed dataset\n",
    "        output_file = file_path.replace(\".csv\", \"_lemmatize.csv\")\n",
    "\n",
    "        # Save the processed dataset (overwrite old data)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed dataset saved as {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset {dataset}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date_pre</th>\n",
       "      <th>end_date_pre</th>\n",
       "      <th>ip_address_pre</th>\n",
       "      <th>duration_sec_pre</th>\n",
       "      <th>response_id_pre</th>\n",
       "      <th>LocationLatitude_pre</th>\n",
       "      <th>LocationLongitude_pre</th>\n",
       "      <th>DistributionChannel_pre</th>\n",
       "      <th>UserLanguage_pre</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>age_pre</th>\n",
       "      <th>gender_pre</th>\n",
       "      <th>occupation_pre</th>\n",
       "      <th>teaching_marketing_pre</th>\n",
       "      <th>teaching_experience_pre</th>\n",
       "      <th>learning_style_pre</th>\n",
       "      <th>learning_format_pre</th>\n",
       "      <th>interaction_preference_pre</th>\n",
       "      <th>trusted_learning_method_pre</th>\n",
       "      <th>ai_familiarity_pre</th>\n",
       "      <th>ddp_familiarity_pre</th>\n",
       "      <th>data_sources_pre</th>\n",
       "      <th>persona_definition_pre</th>\n",
       "      <th>interactive_persona_pre</th>\n",
       "      <th>data_driven_persona_pre</th>\n",
       "      <th>dynamic_persona_pre</th>\n",
       "      <th>tableau_usage_pre</th>\n",
       "      <th>api_usage_pre</th>\n",
       "      <th>ml_application_pre</th>\n",
       "      <th>persona_explanation_pre</th>\n",
       "      <th>confirmation_pre</th>\n",
       "      <th>start_date_post</th>\n",
       "      <th>end_date_post</th>\n",
       "      <th>ip_address_post</th>\n",
       "      <th>duration_sec_post</th>\n",
       "      <th>response_id_post</th>\n",
       "      <th>LocationLatitude_post</th>\n",
       "      <th>LocationLongitude_post</th>\n",
       "      <th>DistributionChannel_post</th>\n",
       "      <th>UserLanguage_post</th>\n",
       "      <th>data_collection_explanation_post</th>\n",
       "      <th>data_analysis_explanation_post</th>\n",
       "      <th>persona_building_explanation_post</th>\n",
       "      <th>evaluation_explanation_post</th>\n",
       "      <th>ai_familiarity_post</th>\n",
       "      <th>ddp_familiarity_post</th>\n",
       "      <th>data_sources_post</th>\n",
       "      <th>persona_definition_post</th>\n",
       "      <th>interactive_persona_post</th>\n",
       "      <th>data_driven_persona_post</th>\n",
       "      <th>dynamic_persona_post</th>\n",
       "      <th>tools_usage_post</th>\n",
       "      <th>api_usage_post</th>\n",
       "      <th>ml_application_post</th>\n",
       "      <th>engagement_experience_post</th>\n",
       "      <th>interaction_quality_post</th>\n",
       "      <th>communication_clarity_post</th>\n",
       "      <th>trustworthiness_post</th>\n",
       "      <th>emotional_response_post</th>\n",
       "      <th>naturalness_post</th>\n",
       "      <th>effectiveness_post</th>\n",
       "      <th>comfort_level_post</th>\n",
       "      <th>personalization_post</th>\n",
       "      <th>mental_effort_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2/13/25 0:10</td>\n",
       "      <td>2/13/25 0:16</td>\n",
       "      <td>193.166.113.18</td>\n",
       "      <td>372</td>\n",
       "      <td>R_8GBoA0i1k6yogS1</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['I', 'do', 'not', 'know']</td>\n",
       "      <td>[\"['I',\", \"'do',\", \"'not',\", \"'know']\"]</td>\n",
       "      <td>[\"['I',\", \"'do',\", \"'not',\", \"'know']\"]</td>\n",
       "      <td>['I', 'do', 'not', 'know']</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:56:11</td>\n",
       "      <td>2025-02-13 1:18:59</td>\n",
       "      <td>193.166.113.18</td>\n",
       "      <td>1368</td>\n",
       "      <td>R_8QL21bpDcdbdtJ4</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['to', 'collect', \"studens'\", 'demographic', '...</td>\n",
       "      <td>['First', 'the', 'data', 'must', 'be', 'cleane...</td>\n",
       "      <td>['After', 'clustering', 'and', 'combining,', '...</td>\n",
       "      <td>['The', 'evaluation', 'can', 'be', 'done', 'e....</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['To', 'communicate', 'the', 'personas', 'to',...</td>\n",
       "      <td>['Because', 'relevant', 'data', 'can', 'be', '...</td>\n",
       "      <td>['They', 'can', 'be', 'used', 'e.g.,', 'in', '...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/13/25 0:09</td>\n",
       "      <td>2/13/25 0:16</td>\n",
       "      <td>193.166.113.32</td>\n",
       "      <td>409</td>\n",
       "      <td>R_8QG2UV0Zv0fKVB5</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1,3,4</td>\n",
       "      <td>2,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['I', 'do', 'not', 'know', 'these', 'tools,', ...</td>\n",
       "      <td>[\"['I',\", \"'have',\", \"'no',\", \"'clue.']\"]</td>\n",
       "      <td>[\"['I',\", \"'have',\", \"'no',\", \"'clue.']\"]</td>\n",
       "      <td>['I', 'have', 'no', 'clue.']</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:56:02</td>\n",
       "      <td>2025-02-13 1:13:21</td>\n",
       "      <td>193.166.113.32</td>\n",
       "      <td>1039</td>\n",
       "      <td>R_8QrAaCxxXwbpxvL</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['User', 'analytics:', 'Google', 'analytics,',...</td>\n",
       "      <td>['Segmentation', 'can', 'be', 'performed', 'so...</td>\n",
       "      <td>['One', 'way', 'to', 'create', 'student', 'per...</td>\n",
       "      <td>['1)', 'I', 'have', 'not', 'been', 'teaching',...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['To', 'illustrate', '(and', 'update)', 'stati...</td>\n",
       "      <td>['I', 'have', 'no', 'clue', 'what', 'is', 'API.']</td>\n",
       "      <td>['To', 'create', 'dynamic', 'personas.']</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2/13/25 0:09</td>\n",
       "      <td>2/13/25 0:17</td>\n",
       "      <td>193.166.113.31</td>\n",
       "      <td>462</td>\n",
       "      <td>R_8tHW5RDZd4yBkcx</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1,3</td>\n",
       "      <td>3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1,3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>['I', \"don't\", 'know.']</td>\n",
       "      <td>[\"['I',\", '\"don\\'t\",', \"'know',\", \"'what',\", \"...</td>\n",
       "      <td>[\"['I',\", '\"don\\'t\",', \"'know.']\"]</td>\n",
       "      <td>['I', \"don't\", 'know.']</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 10:06:34</td>\n",
       "      <td>2025-02-13 10:19:38</td>\n",
       "      <td>193.166.113.31</td>\n",
       "      <td>784</td>\n",
       "      <td>R_2kRKZFGiWHUQidw</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['I', 'need', 'quantitative', 'data,', 'which'...</td>\n",
       "      <td>['Statistical', 'analyses', 'are', 'required.'...</td>\n",
       "      <td>['In', 'this', 'stage,', 'crafting', 'the', 'p...</td>\n",
       "      <td>['This', 'is', 'a', 'crucial', 'step', 'in', '...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['To', 'visualise', 'the', 'persona.']</td>\n",
       "      <td>['I', 'dont', 'know', 'what', 'API', 'stands',...</td>\n",
       "      <td>['I', \"don't\", 'know.']</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2/13/25 0:11</td>\n",
       "      <td>2/13/25 0:17</td>\n",
       "      <td>193.166.113.21</td>\n",
       "      <td>355</td>\n",
       "      <td>R_8462bTis2cTNcii</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1,2,3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2,3,4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>[\"['nan']\"]</td>\n",
       "      <td>[\"['nan']\"]</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:55:51</td>\n",
       "      <td>2025-02-13 1:11:01</td>\n",
       "      <td>193.166.113.21</td>\n",
       "      <td>909</td>\n",
       "      <td>R_2M3diy3SR17ual3</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['Data', 'sources:', 'Attendance,', 'group', '...</td>\n",
       "      <td>['Clustering,', 'matrix', 'for', 'patterns,', ...</td>\n",
       "      <td>['use', 'the', 'data', 'and', 'build', 'repres...</td>\n",
       "      <td>['test', 'within', 'the', 'existing', 'network...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['data', 'analysis']</td>\n",
       "      <td>['to', 'collect', 'data']</td>\n",
       "      <td>['To', 'a', 'great', 'deal.', 'can', 'help', '...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2/13/25 0:09</td>\n",
       "      <td>2/13/25 0:18</td>\n",
       "      <td>193.166.117.7</td>\n",
       "      <td>551</td>\n",
       "      <td>R_824J8ZB9ZGfOu8t</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1,2,4</td>\n",
       "      <td>4</td>\n",
       "      <td>3,4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>['-']</td>\n",
       "      <td>[\"['-']\"]</td>\n",
       "      <td>[\"['-']\"]</td>\n",
       "      <td>['-']</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-13 0:55:35</td>\n",
       "      <td>2025-02-13 1:19:16</td>\n",
       "      <td>193.166.117.7</td>\n",
       "      <td>1421</td>\n",
       "      <td>R_2KPeTVjwHtRdvU7</td>\n",
       "      <td>63.1198</td>\n",
       "      <td>21.6798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['-age,', 'jobs,', 'demographic', 'data,', 'ho...</td>\n",
       "      <td>['-clustering', 'students', 'according', 'to',...</td>\n",
       "      <td>['-making', 'descriptive', 'profile', 'posters...</td>\n",
       "      <td>['-personas', 'should', 'not', 'become', 'stat...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['To', 'make', 'posters', 'of', 'the', 'person...</td>\n",
       "      <td>['For', 'analysis?', 'I', \"don't\", 'know', 'wh...</td>\n",
       "      <td>['To', 'develop', 'personas', 'further', 'when...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  start_date_pre  end_date_pre  ip_address_pre  duration_sec_pre  \\\n",
       "0   2/13/25 0:10  2/13/25 0:16  193.166.113.18               372   \n",
       "1   2/13/25 0:09  2/13/25 0:16  193.166.113.32               409   \n",
       "2   2/13/25 0:09  2/13/25 0:17  193.166.113.31               462   \n",
       "3   2/13/25 0:11  2/13/25 0:17  193.166.113.21               355   \n",
       "4   2/13/25 0:09  2/13/25 0:18   193.166.117.7               551   \n",
       "\n",
       "     response_id_pre  LocationLatitude_pre  LocationLongitude_pre  \\\n",
       "0  R_8GBoA0i1k6yogS1               63.1198                21.6798   \n",
       "1  R_8QG2UV0Zv0fKVB5               63.1198                21.6798   \n",
       "2  R_8tHW5RDZd4yBkcx               63.1198                21.6798   \n",
       "3  R_8462bTis2cTNcii               63.1198                21.6798   \n",
       "4  R_824J8ZB9ZGfOu8t               63.1198                21.6798   \n",
       "\n",
       "   DistributionChannel_pre  UserLanguage_pre  participant_id  age_pre  \\\n",
       "0                        0                 0               1       60   \n",
       "1                        0                 0              23       41   \n",
       "2                        0                 0              22       45   \n",
       "3                        0                 0              19       39   \n",
       "4                        0                 0              15       38   \n",
       "\n",
       "   gender_pre  occupation_pre  teaching_marketing_pre teaching_experience_pre  \\\n",
       "0           1               1                       1                      20   \n",
       "1           1               2                       2                       0   \n",
       "2           1               2                       1                       3   \n",
       "3           1               2                       1                       1   \n",
       "4           2               2                       2                       0   \n",
       "\n",
       "  learning_style_pre learning_format_pre interaction_preference_pre  \\\n",
       "0                  3                 3,4                        3,4   \n",
       "1              1,3,4                 2,4                        3,4   \n",
       "2                1,3                 3,4                        3,4   \n",
       "3            1,2,3,4                 3,4                        3,4   \n",
       "4              1,2,4                   4                        3,4   \n",
       "\n",
       "  trusted_learning_method_pre  ai_familiarity_pre  ddp_familiarity_pre  \\\n",
       "0                       2,3,4                  12                   10   \n",
       "1                       2,3,4                  10                    1   \n",
       "2                       2,3,4                  11                   10   \n",
       "3                       2,3,4                  12                    1   \n",
       "4                           2                  12                    1   \n",
       "\n",
       "  data_sources_pre  persona_definition_pre  interactive_persona_pre  \\\n",
       "0                4                       1                        2   \n",
       "1                4                       1                        2   \n",
       "2              1,3                       1                        5   \n",
       "3                4                       1                        5   \n",
       "4                5                       5                        5   \n",
       "\n",
       "   data_driven_persona_pre  dynamic_persona_pre  \\\n",
       "0                        2                    1   \n",
       "1                        2                    1   \n",
       "2                        2                    5   \n",
       "3                        2                    5   \n",
       "4                        5                    5   \n",
       "\n",
       "                                   tableau_usage_pre  \\\n",
       "0                         ['I', 'do', 'not', 'know']   \n",
       "1  ['I', 'do', 'not', 'know', 'these', 'tools,', ...   \n",
       "2                            ['I', \"don't\", 'know.']   \n",
       "3                                            ['nan']   \n",
       "4                                              ['-']   \n",
       "\n",
       "                                       api_usage_pre  \\\n",
       "0            [\"['I',\", \"'do',\", \"'not',\", \"'know']\"]   \n",
       "1          [\"['I',\", \"'have',\", \"'no',\", \"'clue.']\"]   \n",
       "2  [\"['I',\", '\"don\\'t\",', \"'know',\", \"'what',\", \"...   \n",
       "3                                        [\"['nan']\"]   \n",
       "4                                          [\"['-']\"]   \n",
       "\n",
       "                          ml_application_pre       persona_explanation_pre  \\\n",
       "0    [\"['I',\", \"'do',\", \"'not',\", \"'know']\"]    ['I', 'do', 'not', 'know']   \n",
       "1  [\"['I',\", \"'have',\", \"'no',\", \"'clue.']\"]  ['I', 'have', 'no', 'clue.']   \n",
       "2         [\"['I',\", '\"don\\'t\",', \"'know.']\"]       ['I', \"don't\", 'know.']   \n",
       "3                                [\"['nan']\"]                       ['nan']   \n",
       "4                                  [\"['-']\"]                         ['-']   \n",
       "\n",
       "   confirmation_pre      start_date_post        end_date_post ip_address_post  \\\n",
       "0                 0   2025-02-13 0:56:11   2025-02-13 1:18:59  193.166.113.18   \n",
       "1                 0   2025-02-13 0:56:02   2025-02-13 1:13:21  193.166.113.32   \n",
       "2                 0  2025-02-13 10:06:34  2025-02-13 10:19:38  193.166.113.31   \n",
       "3                 0   2025-02-13 0:55:51   2025-02-13 1:11:01  193.166.113.21   \n",
       "4                 0   2025-02-13 0:55:35   2025-02-13 1:19:16   193.166.117.7   \n",
       "\n",
       "   duration_sec_post   response_id_post  LocationLatitude_post  \\\n",
       "0               1368  R_8QL21bpDcdbdtJ4                63.1198   \n",
       "1               1039  R_8QrAaCxxXwbpxvL                63.1198   \n",
       "2                784  R_2kRKZFGiWHUQidw                63.1198   \n",
       "3                909  R_2M3diy3SR17ual3                63.1198   \n",
       "4               1421  R_2KPeTVjwHtRdvU7                63.1198   \n",
       "\n",
       "   LocationLongitude_post  DistributionChannel_post  UserLanguage_post  \\\n",
       "0                 21.6798                         0                  0   \n",
       "1                 21.6798                         0                  0   \n",
       "2                 21.6798                         0                  0   \n",
       "3                 21.6798                         0                  0   \n",
       "4                 21.6798                         0                  0   \n",
       "\n",
       "                    data_collection_explanation_post  \\\n",
       "0  ['to', 'collect', \"studens'\", 'demographic', '...   \n",
       "1  ['User', 'analytics:', 'Google', 'analytics,',...   \n",
       "2  ['I', 'need', 'quantitative', 'data,', 'which'...   \n",
       "3  ['Data', 'sources:', 'Attendance,', 'group', '...   \n",
       "4  ['-age,', 'jobs,', 'demographic', 'data,', 'ho...   \n",
       "\n",
       "                      data_analysis_explanation_post  \\\n",
       "0  ['First', 'the', 'data', 'must', 'be', 'cleane...   \n",
       "1  ['Segmentation', 'can', 'be', 'performed', 'so...   \n",
       "2  ['Statistical', 'analyses', 'are', 'required.'...   \n",
       "3  ['Clustering,', 'matrix', 'for', 'patterns,', ...   \n",
       "4  ['-clustering', 'students', 'according', 'to',...   \n",
       "\n",
       "                   persona_building_explanation_post  \\\n",
       "0  ['After', 'clustering', 'and', 'combining,', '...   \n",
       "1  ['One', 'way', 'to', 'create', 'student', 'per...   \n",
       "2  ['In', 'this', 'stage,', 'crafting', 'the', 'p...   \n",
       "3  ['use', 'the', 'data', 'and', 'build', 'repres...   \n",
       "4  ['-making', 'descriptive', 'profile', 'posters...   \n",
       "\n",
       "                         evaluation_explanation_post  ai_familiarity_post  \\\n",
       "0  ['The', 'evaluation', 'can', 'be', 'done', 'e....                    4   \n",
       "1  ['1)', 'I', 'have', 'not', 'been', 'teaching',...                    2   \n",
       "2  ['This', 'is', 'a', 'crucial', 'step', 'in', '...                    3   \n",
       "3  ['test', 'within', 'the', 'existing', 'network...                    4   \n",
       "4  ['-personas', 'should', 'not', 'become', 'stat...                    4   \n",
       "\n",
       "   ddp_familiarity_post data_sources_post  persona_definition_post  \\\n",
       "0                     4  All of the above                        0   \n",
       "1                     2  All of the above                        0   \n",
       "2                     4  All of the above                        0   \n",
       "3                     5  All of the above                        0   \n",
       "4                     2  All of the above                        0   \n",
       "\n",
       "   interactive_persona_post  data_driven_persona_post  dynamic_persona_post  \\\n",
       "0                         1                         0                     0   \n",
       "1                         1                         0                     0   \n",
       "2                         2                         0                     1   \n",
       "3                         1                         0                     0   \n",
       "4                         1                         0                     0   \n",
       "\n",
       "                                    tools_usage_post  \\\n",
       "0  ['To', 'communicate', 'the', 'personas', 'to',...   \n",
       "1  ['To', 'illustrate', '(and', 'update)', 'stati...   \n",
       "2             ['To', 'visualise', 'the', 'persona.']   \n",
       "3                               ['data', 'analysis']   \n",
       "4  ['To', 'make', 'posters', 'of', 'the', 'person...   \n",
       "\n",
       "                                      api_usage_post  \\\n",
       "0  ['Because', 'relevant', 'data', 'can', 'be', '...   \n",
       "1  ['I', 'have', 'no', 'clue', 'what', 'is', 'API.']   \n",
       "2  ['I', 'dont', 'know', 'what', 'API', 'stands',...   \n",
       "3                          ['to', 'collect', 'data']   \n",
       "4  ['For', 'analysis?', 'I', \"don't\", 'know', 'wh...   \n",
       "\n",
       "                                 ml_application_post  \\\n",
       "0  ['They', 'can', 'be', 'used', 'e.g.,', 'in', '...   \n",
       "1           ['To', 'create', 'dynamic', 'personas.']   \n",
       "2                            ['I', \"don't\", 'know.']   \n",
       "3  ['To', 'a', 'great', 'deal.', 'can', 'help', '...   \n",
       "4  ['To', 'develop', 'personas', 'further', 'when...   \n",
       "\n",
       "   engagement_experience_post  interaction_quality_post  \\\n",
       "0                           5                         1   \n",
       "1                           4                         2   \n",
       "2                           3                         4   \n",
       "3                           6                         6   \n",
       "4                           4                         2   \n",
       "\n",
       "   communication_clarity_post  trustworthiness_post  emotional_response_post  \\\n",
       "0                           7                     2                        2   \n",
       "1                           4                     4                        2   \n",
       "2                           6                     6                        1   \n",
       "3                           6                     4                        2   \n",
       "4                           3                     3                        1   \n",
       "\n",
       "   naturalness_post  effectiveness_post  comfort_level_post  \\\n",
       "0                 1                   6                   6   \n",
       "1                 4                   4                   2   \n",
       "2                 4                   5                   3   \n",
       "3                 3                   6                   1   \n",
       "4                 4                   3                   6   \n",
       "\n",
       "   personalization_post  mental_effort_post  \n",
       "0                     1                   4  \n",
       "1                     2                   6  \n",
       "2                     4                   5  \n",
       "3                     4                   4  \n",
       "4                     1                   6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the cleaned dataset\n",
    "# Ensure all columns are displayed\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
