{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Bard API (PaLM 2) for Survey Answer Classification  \n",
    "Since you chose Google Bard API (PaLM 2), we will use it to classify survey responses as High, Medium, or Low based on correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-generativeai\n",
    "import google.generativeai as genai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google PaLM 2 API key\n",
    "genai.configure(api_key=\"AIzaSyBeHsGmsYYmSMHZUuHc54HvUulIQUDzMxU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define correct answers for numeric questions\n",
    "numeric_correct_answers = {\n",
    "    \"data_sources_pre\": 4,\n",
    "    \"persona_definition_pre\": 1,\n",
    "    \"interactive_persona_pre\": 2,\n",
    "    \"data_driven_persona_pre\": 10,\n",
    "    \"dynamic_persona_pre\": 10,\n",
    "    \"persona_definition_post\": 10,\n",
    "    \"interactive_persona_post\": 10,\n",
    "    \"data_driven_persona_post\": 10,\n",
    "    \"dynamic_persona_post\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify numeric answers\n",
    "def classify_numeric_answer(column, response):\n",
    "    \"\"\"\n",
    "    Classifies numeric answers as Correct or Not Correct based on predefined correct values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response_value = float(response)  # Convert response to float\n",
    "        correct_value = numeric_correct_answers.get(column, None)\n",
    "\n",
    "        if correct_value is not None:\n",
    "            return \"Correct\" if response_value == correct_value else \"Not Correct\"\n",
    "        else:\n",
    "            return \"No Classification\"\n",
    "    except ValueError:\n",
    "        return \"Invalid\"  # Handle cases where response is not a valid number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grade responses using the AI\n",
    "def grade_answer(question, response):\n",
    "    \"\"\"\n",
    "    Uses Google Bard API (PaLM 2) to classify survey responses as High, Medium, or Low correctness.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI grader for a survey. Your job is to evaluate the correctness of participant answers.\n",
    "\n",
    "    - HIGH: If the answer is fully correct and detailed.\n",
    "    - MEDIUM: If the answer is somewhat correct but lacks detail.\n",
    "    - LOW: If the answer is incorrect or unrelated.\n",
    "\n",
    "    Question: {question}\n",
    "    Answer: {response}\n",
    "\n",
    "    How would you score this answer? (Just reply with 'High', 'Medium', or 'Low')\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-pro\")\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error grading response: {e}\")\n",
    "        return \"Error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare pre/post responses and assign performance labels\n",
    "def compare_performance(pre_label, post_label):\n",
    "    \"\"\"\n",
    "    Assigns a performance change label based on pre-survey and post-survey correctness.\n",
    "    \"\"\"\n",
    "    label_order = {\"Low\": 1, \"Medium\": 2, \"High\": 3}  # Convert labels to numeric values for comparison\n",
    "\n",
    "    if pre_label not in label_order or post_label not in label_order:\n",
    "        return \"Error\"  # Handle unexpected labels\n",
    "\n",
    "    if label_order[post_label] > label_order[pre_label]:\n",
    "        return \"Improved\"\n",
    "    elif label_order[post_label] < label_order[pre_label]:\n",
    "        return \"Declined\"\n",
    "    else:\n",
    "        return \"Same\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load survey dataset\n",
    "file_path = \"all_numeric_survey_with questions.csv\"  # Replace with your actual survey file\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all columns to be graded (correctness classification)\n",
    "all_columns = [\"Explain how data-driven personas development could be used to create student personas for a marketing course that you are teaching (or you have participated in as a student). Be as specific as you can. Cover the following aspect in this question: - data collection (e.g., what data to collect, how to collect, how much)\", \n",
    "               \"Explain how data-driven personas development could be used to create student personas for a marketing course that you are teaching (or you have participated in as a student). Be as specific as you can. Cover the following aspect in this question: - data analysis (e.g., how to segment the data, what methods to use)\", \n",
    "               \"Explain how data-driven personas development could be used to create student personas for a marketing course that you are teaching (or you have participated in as a student). Be as specific as you can. Cover the following aspect in this question: - persona building (e.g., how to create persona profiles)\", \n",
    "               \"Explain how data-driven personas development could be used to create student personas for a marketing course that you are teaching (or you have participated in as a student). Be as specific as you can. Cover the following aspect in this question: - and evaluation (e.g., how to evaluate that the personas are accurate and useful)\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify which columns belong to pre-survey and post-survey\n",
    "pre_columns = [\"For what purposes can tools like Tableau, Power BI, or Python libraries be used in persona development?\", \n",
    "               \"In persona development, why would you use APIs?  (pre)\", \n",
    "               \"For what purposes can machine learning algorithms be used in persona development?  (pre)\"] \n",
    "\n",
    "\n",
    "post_columns = [\"For what purposes can tools like Canva or Powerpoint be used in persona development? (post)\", \n",
    "                \"In persona development, why would you use APIs? (post)\", \n",
    "                \"For what purposes can machine learning algorithms be used in persona development? (post)\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame to store graded responses\n",
    "graded_df = df.copy()\n",
    "\n",
    "# Ensure the length of both lists match for performance classification\n",
    "if len(pre_columns) != len(post_columns):\n",
    "    raise ValueError(\"Pre-survey and post-survey column lists must have the same length.\")\n",
    "\n",
    "# Grade all columns (Correctness: HIGH, MEDIUM, LOW)\n",
    "for col in all_columns:\n",
    "    question = col.replace(\"_\", \" \").title()  # Convert column name to a readable question\n",
    "    print(f\"Grading responses for: {question}...\")\n",
    "    graded_df[f\"{col}_Grade\"] = df[col].astype(str).apply(lambda response: grade_answer(question, response))\n",
    "\n",
    "# Compare pre/post survey responses only for selected columns (Performance: IMPROVED, SAME, DECLINED)\n",
    "for pre_col, post_col in zip(pre_columns, post_columns):\n",
    "    print(f\"Comparing performance for: {pre_col} â†’ {post_col}...\")\n",
    "    graded_df[f\"{pre_col}_Performance\"] = graded_df.apply(\n",
    "        lambda row: compare_performance(row[f\"{pre_col}_Grade\"], row[f\"{post_col}_Grade\"]), axis=1\n",
    "    )\n",
    "\n",
    "# Save the graded responses\n",
    "output_file = \"survey_with_grades.csv\"\n",
    "graded_df.to_csv(output_file, index=False)\n",
    "print(f\"Graded responses saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Manually specify which columns belong to pre-survey and post-survey\n",
    "pre_columns = [\"column_1_pre\", \"column_2_pre\", \"column_3_pre\"]  # Replace with actual pre-survey column names\n",
    "post_columns = [\"column_1_post\", \"column_2_post\", \"column_3_post\"]  # Replace with corresponding post-survey column names\n",
    "\n",
    "graded_df = df.copy()  # Create a new DataFrame to store graded responses\n",
    "\n",
    "# Ensure the length of both lists match\n",
    "if len(pre_columns) != len(post_columns):\n",
    "    raise ValueError(\"Pre-survey and post-survey column lists must have the same length.\")\n",
    "\n",
    "# Loop through each pre/post column pair\n",
    "for pre_col, post_col in zip(pre_columns, post_columns):\n",
    "    # Extract the question from column name\n",
    "    question = pre_col.replace(\"_\", \" \").title()  # Convert column name to a readable question\n",
    "\n",
    "    print(f\"Grading responses for: {question}...\")\n",
    "\n",
    "    # Grade pre-survey responses\n",
    "    graded_df[f\"{pre_col}_Grade\"] = df[pre_col].astype(str).apply(lambda response: grade_answer(question, response))\n",
    "\n",
    "    # Grade post-survey responses\n",
    "    graded_df[f\"{post_col}_Grade\"] = df[post_col].astype(str).apply(lambda response: grade_answer(question, response))\n",
    "\n",
    "    # Compare pre/post grades to determine performance change\n",
    "    graded_df[f\"{pre_col}_Performance\"] = graded_df.apply(\n",
    "        lambda row: compare_performance(row[f\"{pre_col}_Grade\"], row[f\"{post_col}_Grade\"]), axis=1\n",
    "    )\n",
    "\n",
    "# Save the graded responses\n",
    "output_file = \"survey_with_grades.csv\"\n",
    "graded_df.to_csv(output_file, index=False)\n",
    "print(f\"Graded responses saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
