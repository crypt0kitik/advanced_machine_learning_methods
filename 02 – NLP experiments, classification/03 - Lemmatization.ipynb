{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had an issue with NLTK\n",
    "# that is I opted for spacy\n",
    "import spacy\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the spaCy model if not already installed\n",
    "def download_spacy_model():\n",
    "    try:\n",
    "        spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(\"Downloading 'en_core_web_sm' model...\")\n",
    "        subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
    "        print(\"Download complete!\")\n",
    "\n",
    "# Ensure the spaCy model is available\n",
    "download_spacy_model()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Function to lemmatize text using spaCy.\n",
    "    :param text: str, input text\n",
    "    :return: str, lemmatized text\n",
    "    \"\"\"\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return \"\"\n",
    "\n",
    "    doc = nlp(text)  # Process the text using spaCy\n",
    "    return \" \".join([token.lemma_ for token in doc])  # Apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datasets = [\n",
    "    \"dataset_word_token_stopwords.csv\",\n",
    "    \"dataset_subword_token_stopwords.csv\",\n",
    "    \"dataset_sentence_token_stopwords.csv\",\n",
    "    \"dataset_bert_token_stopwords.csv\",\n",
    "    \"dataset_tiktoken_token_stopwords.csv\",\n",
    "    \"dataset_whitespace_token_stopwords.csv\"\n",
    "] \n",
    "\n",
    "text_columns = [\"tableau_usage_pre\", \"api_usage_pre\", \"ml_application_pre\",\n",
    "                    \"persona_explanation_pre\", \"api_usage_pre\", \"ml_application_pre\",\n",
    "                    \"data_collection_explanation_post\", \"data_analysis_explanation_post\", \"persona_building_explanation_post\",\n",
    "                    \"evaluation_explanation_post\", \"tools_usage_post\", \"api_usage_post\",\n",
    "                    \"ml_application_post\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dataset_word_token_stopwords.csv loaded successfully!\n",
      "Processed dataset saved as dataset_word_token_stopwords_lemmatize.csv\n",
      "Dataset dataset_subword_token_stopwords.csv loaded successfully!\n",
      "Processed dataset saved as dataset_subword_token_stopwords_lemmatize.csv\n",
      "Dataset dataset_sentence_token_stopwords.csv loaded successfully!\n",
      "Processed dataset saved as dataset_sentence_token_stopwords_lemmatize.csv\n",
      "Dataset dataset_bert_token_stopwords.csv loaded successfully!\n",
      "Processed dataset saved as dataset_bert_token_stopwords_lemmatize.csv\n",
      "Dataset dataset_tiktoken_token_stopwords.csv loaded successfully!\n",
      "Processed dataset saved as dataset_tiktoken_token_stopwords_lemmatize.csv\n",
      "Dataset dataset_whitespace_token_stopwords.csv loaded successfully!\n",
      "Processed dataset saved as dataset_whitespace_token_stopwords_lemmatize.csv\n"
     ]
    }
   ],
   "source": [
    "# Process each dataset\n",
    "for dataset in datasets:\n",
    "    try:\n",
    "        file_path = dataset  # Assign dataset name directly\n",
    "\n",
    "        # Load dataset\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset {dataset} loaded successfully!\")\n",
    "\n",
    "        # Apply lemmatization to each column in the list\n",
    "        for col in text_columns:\n",
    "            if col in df.columns:  # Ensure column exists before processing\n",
    "                df[col] = df[col].astype(str).apply(lemmatize_text)\n",
    "\n",
    "        # Generate a unique name for the processed dataset\n",
    "        output_file = file_path.replace(\".csv\", \"_lemmatize.csv\")\n",
    "\n",
    "        # Save the processed dataset (overwrite old data)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed dataset saved as {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset {dataset}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
