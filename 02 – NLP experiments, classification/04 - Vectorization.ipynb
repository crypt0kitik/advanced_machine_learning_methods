{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scipy scikit-learn spacy gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emiliiazemskova/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack, csr_matrix, save_npz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import spacy\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 1.6 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy English model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of datasets to process\n",
    "datasets = [\n",
    "    \"01.1_df_word_token_stopwords_lemmatize.csv\",\n",
    "    \"01.2_df_subword_token_stopwords_lemmatize.csv\",\n",
    "    \"01.3_df_sentence_token_stopwords_lemmatize.csv\",\n",
    "    \"01.4_df_bert_token_stopwords_lemmatize.csv\",\n",
    "    \"01.5_df_tiktoken_token_stopwords_lemmatize.csv\",\n",
    "    \"01.6_df_whitespace_token_stopwords_lemmatize.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column groups for text processing\n",
    "short_text_columns = [\n",
    "    \"api_usage_pre\", \"tableau_usage_pre\", \"ml_application_pre\", \"persona_explanation_pre\",\n",
    "    \"tools_usage_post\", \"api_usage_post\", \"ml_application_post\"\n",
    "]\n",
    "long_text_columns = [\n",
    "    \"data_collection_explanation_post\", \"data_analysis_explanation_post\",\n",
    "    \"persona_building_explanation_post\", \"evaluation_explanation_post\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to track processing results\n",
    "saved_files = []\n",
    "failed_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_with_glove(sentence, model, vector_size=50):\n",
    "    \"\"\"Convert a tokenized sentence into a GloVe word embedding vector.\"\"\"\n",
    "    words = [token.text for token in nlp(sentence) if token.text in model]\n",
    "    if not words:\n",
    "        return np.zeros(vector_size)  # Return zero vector if no known words are found\n",
    "    return np.mean([model[word] for word in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_survey(file_name, use_count_vectorizer=False, save=True):\n",
    "    \"\"\"Vectorizes a survey dataset using TF-IDF (or CountVectorizer) and GloVe embeddings, and saves output.\"\"\"\n",
    "    try:\n",
    "        # Ensure file exists before proceeding\n",
    "        if not os.path.exists(file_name):\n",
    "            raise FileNotFoundError(f\"File not found: {file_name}\")\n",
    "\n",
    "        df = pd.read_csv(file_name)\n",
    "        print(f\"Processing survey: {file_name}\")\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        missing_cols = [col for col in short_text_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing short_text_columns: {missing_cols}\")\n",
    "\n",
    "        missing_cols = [col for col in long_text_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing long_text_columns: {missing_cols}\")\n",
    "\n",
    "        # Use either TF-IDF or CountVectorizer based on parameter\n",
    "        if use_count_vectorizer:\n",
    "            vectorizer = CountVectorizer()\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(norm=None)  # Prevents negative values\n",
    "\n",
    "        short_response_vectors = vectorizer.fit_transform(\n",
    "            df[short_text_columns].fillna(\"\").agg(\" \".join, axis=1)\n",
    "        )\n",
    "\n",
    "        # GloVe Embeddings for long responses\n",
    "        df[\"glove_vector\"] = df[long_text_columns].fillna(\"\").agg(\" \".join, axis=1).apply(\n",
    "            lambda x: vectorize_sentence_with_glove(x, glove_model)\n",
    "        )\n",
    "\n",
    "        # Convert GloVe vectors to sparse format\n",
    "        glove_vectors = csr_matrix(np.vstack(df[\"glove_vector\"]))\n",
    "\n",
    "        # Stack both feature matrices\n",
    "        final_feature_matrix = hstack([short_response_vectors, glove_vectors])\n",
    "\n",
    "        print(f\"Vectorization complete for: {file_name}, Shape: {final_feature_matrix.shape}\")\n",
    "\n",
    "        # Ensure the final matrix is valid before saving\n",
    "        if final_feature_matrix.shape[0] == 0 or final_feature_matrix.shape[1] == 0:\n",
    "            raise ValueError(f\"Vectorized matrix is empty for {file_name}\")\n",
    "\n",
    "        # Save vectorized output in the same directory as the original file\n",
    "        if save:\n",
    "            output_file = file_name.replace(\".csv\", \"_vectorized.npz\")\n",
    "            save_npz(output_file, final_feature_matrix)\n",
    "            saved_files.append(output_file)\n",
    "            print(f\"Saved vectorized features to: {output_file}\")\n",
    "\n",
    "        return final_feature_matrix\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "        failed_files.append(file_name)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing survey: 01.1_df_word_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.1_df_word_token_stopwords_lemmatize.csv, Shape: (26, 339)\n",
      "Saved vectorized features to: 01.1_df_word_token_stopwords_lemmatize_vectorized.npz\n",
      "Processing survey: 01.2_df_subword_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.2_df_subword_token_stopwords_lemmatize.csv, Shape: (26, 579)\n",
      "Saved vectorized features to: 01.2_df_subword_token_stopwords_lemmatize_vectorized.npz\n",
      "Processing survey: 01.3_df_sentence_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.3_df_sentence_token_stopwords_lemmatize.csv, Shape: (26, 348)\n",
      "Saved vectorized features to: 01.3_df_sentence_token_stopwords_lemmatize_vectorized.npz\n",
      "Processing survey: 01.4_df_bert_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.4_df_bert_token_stopwords_lemmatize.csv, Shape: (26, 360)\n",
      "Saved vectorized features to: 01.4_df_bert_token_stopwords_lemmatize_vectorized.npz\n",
      "Processing survey: 01.5_df_tiktoken_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.5_df_tiktoken_token_stopwords_lemmatize.csv, Shape: (26, 725)\n",
      "Saved vectorized features to: 01.5_df_tiktoken_token_stopwords_lemmatize_vectorized.npz\n",
      "Processing survey: 01.6_df_whitespace_token_stopwords_lemmatize.csv\n",
      "Vectorization complete for: 01.6_df_whitespace_token_stopwords_lemmatize.csv, Shape: (26, 341)\n",
      "Saved vectorized features to: 01.6_df_whitespace_token_stopwords_lemmatize_vectorized.npz\n"
     ]
    }
   ],
   "source": [
    "# Process all datasets\n",
    "for dataset in datasets:\n",
    "    process_survey(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Summary ---\n",
      "Total files processed: 6\n",
      "Successfully saved: 6 files\n",
      "   - 01.1_df_word_token_stopwords_lemmatize_vectorized.npz\n",
      "   - 01.2_df_subword_token_stopwords_lemmatize_vectorized.npz\n",
      "   - 01.3_df_sentence_token_stopwords_lemmatize_vectorized.npz\n",
      "   - 01.4_df_bert_token_stopwords_lemmatize_vectorized.npz\n",
      "   - 01.5_df_tiktoken_token_stopwords_lemmatize_vectorized.npz\n",
      "   - 01.6_df_whitespace_token_stopwords_lemmatize_vectorized.npz\n"
     ]
    }
   ],
   "source": [
    "# Function to print summary of saved files\n",
    "def print_summary():\n",
    "    print(\"\\n--- Processing Summary ---\")\n",
    "    print(f\"Total files processed: {len(saved_files) + len(failed_files)}\")\n",
    "    print(f\"Successfully saved: {len(saved_files)} files\")\n",
    "    for file in saved_files:\n",
    "        print(f\"   - {file}\")\n",
    "    if failed_files:\n",
    "        print(f\"\\nFailed to process {len(failed_files)} files:\")\n",
    "        for file in failed_files:\n",
    "            print(f\"   - {file}\")\n",
    "\n",
    "# Print the final summary\n",
    "print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
